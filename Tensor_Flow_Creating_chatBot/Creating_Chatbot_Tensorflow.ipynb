{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Creating Chatbot Using Tensorflow </center> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is TensorFlow ?\n",
    "1. TensorFlow is A Python Library Used To Implement Deep Network\n",
    "2. In Tensorflow , Computation Is Approached As Dataflow Grapgh \n",
    "\n",
    "\n",
    "<img src=\"Tensor_Flow_Explain.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Do we need a chatbot?1\n",
    "<img src=\"need.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does Chatbot Work?\n",
    "<img src=\"work.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP) layer \n",
    "1. Mapping The Given Input in Natural Language Into Useful Representations.\n",
    "2. Analyzing different aspects of The Language.\n",
    "\n",
    "<img src=\"NLP.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base/CMS\n",
    "<img src=\"CSM.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Store\n",
    "1. Data Required To Train the Bot\n",
    "2. User's Chat comes to bot Once it's deployed.\n",
    "\n",
    "<img src=\"Data_Store.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\"> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"Data_Store1.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Point TO be Considered while Creating a Chatbot\n",
    "- Volume of Conversation\n",
    "- Consistency Of the agents\n",
    "- Training the Agents\n",
    "- Length Of Conversations\n",
    "- Complexity of incoming & outgoing question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of Chatbot\n",
    "1. Real Estate\n",
    "2. Flight Tickets\n",
    "3. Product SElection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Of Chatbot\n",
    "1. Natural Language Processing \n",
    "2. Neural Network\n",
    "3. Deep Learning\n",
    "4. TensorFlow\n",
    "\n",
    "\n",
    "<img src=\"NLP1.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Natural Language Processing \n",
    "- A Part Of AI which deals with Understanding of Human Language/ Natural Language By A Program. <br>\n",
    "- Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. It involves the development of algorithms and models that enable computers to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.<br>\n",
    "\n",
    "NLP encompasses a wide range of tasks and applications, including:\n",
    "\n",
    "1. <b>Language Understanding:</b> This involves tasks such as text classification, sentiment analysis, named entity recognition, and part-of-speech tagging. These tasks help computers extract structured information from unstructured text.\n",
    "\n",
    "2. <b>Language Generation: </b> NLP can be used to generate human-like text, such as in chatbots, language translation, and text summarization. These systems use algorithms to produce coherent and contextually appropriate sentences.\n",
    "\n",
    "3. <b>Machine Translation:</b> NLP plays a crucial role in machine translation, allowing computers to automatically translate text from one language to another, such as in services like Google Translate.\n",
    "\n",
    "4. <b>Speech Recognition:</b> NLP enables computers to convert spoken language into written text. This technology is used in voice assistants like Siri and Google Assistant.\n",
    "\n",
    "5. <b>Question Answering:</b> NLP systems can answer questions posed in natural language, often by extracting relevant information from large datasets or documents.\n",
    "\n",
    "6. <b>Language Models: </b>NLP models like GPT-3 (which I am based on) are designed to generate human-like text and have been used for a variety of applications, from content creation to code generation.\n",
    "\n",
    "7. <b>Information Retrieval:</b> NLP techniques are used in search engines to retrieve relevant information from a vast amount of text data.\n",
    "\n",
    "8. <b> Sentiment Analysis:</b> NLP can determine the sentiment or emotion expressed in a piece of text, which is valuable for understanding public opinion and customer feedback.\n",
    "\n",
    "NLP involves a combination of linguistics, computer science, and machine learning techniques. It often requires preprocessing steps like tokenization (breaking text into words or phrases), stemming (reducing words to their root forms), and lemmatization (reducing words to their base or dictionary forms) to make the text more amenable to analysis. Machine learning algorithms, including neural networks, are commonly used in NLP to model the relationships between words and phrases and make predictions or generate text.\n",
    "- <img src=\"Application.webp\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network :\n",
    "\n",
    "- Artificial Neural Network (ANNs) Are computing Systems inspired BY the biological Neural Networks That constitute animal Brain. Such Systems Learn to do tasks by Considering \n",
    "  Exapmle: Generally Without Task Specific Programming.\n",
    "\n",
    "- Deep Learning Uses Deep Networks Which Are Nothing But Neural Network With multiple Hidden Layers.\n",
    "\n",
    "<img src=\"Neural_Network.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Deep Learning Works?\n",
    "\n",
    "<img src=\"Deep_Learning.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Of Chatbot\n",
    "<img src=\"Workflow.png\" alt=\"Tensor_Flow_Explain\" width=\"500\" Hight=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "with open('dialogs1.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        question, answer = line.strip().split('\\t')\n",
    "        questions.append(question)\n",
    "        answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and create vocabulary\n",
    "special_tokens = ['<start>', '<end>', '']\n",
    "tokenizer = tf.keras.layers.TextVectorization(max_tokens=100, output_sequence_length=10)\n",
    "tokenizer.adapt(special_tokens + questions + answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to tokenized sequences\n",
    "questions_seq = tokenizer(questions)\n",
    "answers_seq = tokenizer(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 5s 34ms/step - loss: 3.4968\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 3.2417\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 2.7371\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 2.1566\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 2.0823\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 2.0259\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 1.9281\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 1.8703\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1.8317\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 1.7509\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1.6780\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 1.6083\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 1.5296\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1.4270\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 1.3243\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 1.2102\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 1.1104\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 1.0198\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.9210\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.8549\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.7503\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6520\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.5467\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4600\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.3975\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.3214\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.2579\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2387\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.2033\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.1734\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.1610\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.1268\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.1099\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0940\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0847\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0656\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0594\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0504\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.0440\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0394\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0349\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0303\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0280\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.0255\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0235\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.0215\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0199\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0186\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0173\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22695d00d00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sequence-to-sequence model\n",
    "input_seq = tf.keras.layers.Input(shape=(None,))\n",
    "encoder = tf.keras.layers.Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=256)(input_seq)\n",
    "encoder_output, encoder_state = tf.keras.layers.GRU(256, return_state=True)(encoder)\n",
    "\n",
    "decoder_input = tf.keras.layers.Input(shape=(None,))\n",
    "decoder_embed = tf.keras.layers.Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=256)(decoder_input)\n",
    "decoder_output, _ = tf.keras.layers.GRU(256, return_sequences=True, return_state=True)(\n",
    "    decoder_embed, initial_state=encoder_state\n",
    ")\n",
    "decoder_logits = tf.keras.layers.Dense(len(tokenizer.get_vocabulary()), activation='softmax')(decoder_output)\n",
    "\n",
    "seq2seq_model = tf.keras.models.Model([input_seq, decoder_input], decoder_logits)\n",
    "\n",
    "# Compile the model\n",
    "seq2seq_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the model (dummy data, adjust for real dataset)\n",
    "seq2seq_model.fit([questions_seq, answers_seq[:, :-1]], answers_seq[:, 1:], epochs=50, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    if not input_text.strip():\n",
    "        return \"Please provide a valid input.\"\n",
    "\n",
    "    input_seq = tokenizer([input_text])\n",
    "    decoder_input = tf.convert_to_tensor([[tokenizer.get_vocabulary().index('<start>')]])\n",
    "\n",
    "    output = []\n",
    "    while True:\n",
    "        logits = seq2seq_model([input_seq, decoder_input])\n",
    "        token_id = np.argmax(logits[0, -1, :])\n",
    "        if token_id == tokenizer.get_vocabulary().index('<end>') or len(output) >= 10:\n",
    "            break\n",
    "        output.append(token_id)\n",
    "        decoder_input = np.array([[token_id]])  # Update decoder input for next time step\n",
    "\n",
    "    response_seq = [output]\n",
    "    response_text = tokenizer.sequences_to_texts(response_seq)[0]\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    if not input_text.strip():\n",
    "        return \"Please provide a valid input.\"\n",
    "\n",
    "    input_seq = tokenizer([input_text])\n",
    "    start_token_id = tokenizer.get_vocabulary().index('<start>')\n",
    "    decoder_input = tf.convert_to_tensor([[start_token_id]])\n",
    "\n",
    "    output = []\n",
    "    while True:\n",
    "        logits = seq2seq_model([input_seq, decoder_input])\n",
    "        token_id = np.argmax(logits[0, -1, :])\n",
    "        if token_id == tokenizer.get_vocabulary().index('<end>') or len(output) >= 10:\n",
    "            break\n",
    "        output.append(token_id)\n",
    "        decoder_input = np.array([[token_id]])  # Update decoder input for the next time step\n",
    "\n",
    "    response_seq = [output]\n",
    "    response_text = tokenizer.sequences_to_texts(response_seq)[0]\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Please provide a valid input.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'<start>' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mif\u001b[39;00m user_input\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m response \u001b[39m=\u001b[39m generate_response(user_input)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mChatbot:\u001b[39m\u001b[39m\"\u001b[39m, response)\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(input_text)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mPlease provide a valid input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m input_seq \u001b[39m=\u001b[39m tokenizer([input_text])\n\u001b[1;32m----> 6\u001b[0m start_token_id \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mget_vocabulary()\u001b[39m.\u001b[39;49mindex(\u001b[39m'\u001b[39;49m\u001b[39m<start>\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m decoder_input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor([[start_token_id]])\n\u001b[0;32m      9\u001b[0m output \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: '<start>' is not in list"
     ]
    }
   ],
   "source": [
    "# Test the chatbot\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    response = generate_response(user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
